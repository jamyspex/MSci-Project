{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Large Eddy Simulator for Urban Flows\n",
    "\n",
    "The code in `src` in this repository has support for MPI and Nested Grids. OpenCL code for GPU and CPU can be generated from this code as well.\n",
    "\n",
    "For more details see [Nested-LES.ipynb](http://localhost:7777/notebooks/Nested-LES.ipynb)\n",
    "\n",
    "2018-09-13\n",
    "\n",
    "Aims: \n",
    "\n",
    "- Validate MPI CPU code without nesting, against non-MPI CPU code, specifically to test\n",
    "- Validate MPI CPU code without nesting, specifically to test\n",
    "    MPI_NEW_WV \n",
    "    WV_NEW\n",
    "    MPI_NEW_WV and WV_NEW\n",
    "    \n",
    "- Validate GPU code against MPI CPU code without nesting.\n",
    "\n",
    "## Validation\n",
    "\n",
    "Short run (100 time steps, then 500), compare uspd/vspd and pressure/vel at centre\n",
    "\n",
    "$$    \n",
    "uspd_{i,j}=\\sqrt{u_{i,j,1}^{2}+\\frac{1}{2}\\left(\\frac{(v_{i,j-1,1}+v_{i,j,1})\\Delta x_{i+1}+(v_{i+1,j-1,1}+v_{i+1,j,1})\\Delta x{}_{i}}{\\Delta x{}_{i}+\\Delta x{}_{i+1}}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "And similar for _vspd_. The number of time steps is of course too small: the LES outputs only one point every 1000 steps.\n",
    "\n",
    "Script:\n",
    "\n",
    "    ./build_and_run_WV_MPI_no_nesting_DPRI.sh 2 2\n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### STATUS \n",
    "\n",
    "- The code without WV_NEW or MPI_NEW_WV now compiles and runs\n",
    "- There seems to be no difference between baseline and MPI_NEW_WV, but I observe that 2 runs do not produce the same pressures\n",
    "\n",
    "### Memory footprint reduction\n",
    "\n",
    "- After splitting out `WV_NEW` functionality with additional `WV_NEW_FEEDBF`,`WV_NEW_LES`,`WV_NEW_LES2`,`WV_NEW_VEL2` and `WV_NEW_VELFG`, the new functionality works. The combined effect is to eliminate all `*mask`, `cov*`, `nou*` and `diu*` arrays. The full picture:\n",
    "\n",
    "* Before: 73 3-D arrays, 16 for aveflow\n",
    "\n",
    "        #ifdef I_AVEFLOW\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avel\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avep\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avesm\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avesmsm\n",
    "            real(kind=4), dimension(ip,kp)  :: avesu\n",
    "            real(kind=4), dimension(ip,kp)  :: avesuu\n",
    "            real(kind=4), dimension(ip,kp)  :: avesv\n",
    "            real(kind=4), dimension(ip,kp)  :: avesvv\n",
    "            real(kind=4), dimension(ip,kp)  :: avesw\n",
    "            real(kind=4), dimension(ip,kp)  :: avesww\n",
    "            real(kind=4), dimension(ip,jp,0:kp)  :: aveu\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: aveuu\n",
    "            real(kind=4), dimension(ip,jp,0:kp)  :: avev\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avevv\n",
    "            real(kind=4), dimension(ip+1,jp,0:kp+2)  :: avew\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: aveww\n",
    "        #endif    \n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1,0:kp+1)  :: amask1\n",
    "            real(kind=4), dimension(-1:ip+1,0:jp+1,0:kp+1)  :: bmask1\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,0:kp+1)  :: cmask1\n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1,0:kp+1)  :: dmask1\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: cn1\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: cov1\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov2\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov3\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov4\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: cov5\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov6\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov7\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov8\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: cov9\n",
    "            real(kind=4), dimension(0:ip,jp,kp)  :: dfu1\n",
    "            real(kind=4), dimension(ip,0:jp,kp)  :: dfv1\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: dfw1\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: diu1\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu2\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu3\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu4\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: diu5\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu6\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu7\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu8\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: diu9\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: f\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: g\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: h\n",
    "\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: fold\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: gold\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: hold\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: fx\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: fy\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: fz\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: nou1\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou2\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou3\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou4\n",
    "            real(kind=4), dimension(-1:ip+2,0:jp+2,0:kp+2)  :: nou5\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou6\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou7\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou8\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+2)  :: nou9\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+1)  :: p\n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1,0:kp+1)  :: rhs\n",
    "            real(kind=4), dimension(-1:ip+1,-1:jp+1,0:kp+1)  :: sm\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,0:kp+1)  :: u\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: usum\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: uwfx\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,0:kp+1)  :: v\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: vsum\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,-1:kp+1)  :: w\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: wsum\n",
    "            real(kind=4), dimension(-1:ipmax+1,-1:jpmax+1)  :: zbm\n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1)  :: uspd\n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1)  :: vspd\n",
    "\n",
    "* After: 26 3-D arrays, 9 for aveflow\n",
    "\n",
    "        #ifdef I_AVEFLOW\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avep\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avesm\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avesmsm\n",
    "            real(kind=4), dimension(ip,jp,0:kp)  :: aveu\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: aveuu\n",
    "            real(kind=4), dimension(ip,jp,0:kp)  :: avev\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: avevv\n",
    "            real(kind=4), dimension(ip+1,jp,0:kp+2)  :: avew\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: aveww\n",
    "        #endif    \n",
    "\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: f\n",
    "\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: fold\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: g\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: gold\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: h\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: hold\n",
    "        #ifndef TWINNED_BUFFER\n",
    "            real(kind=4), dimension(0:ip+2,0:jp+2,0:kp+1)  :: p\n",
    "        #else\n",
    "            real(kind=4), dimension(0:1,0:ip+2,0:jp+2,0:kp+1)  :: p\n",
    "        #endif\n",
    "            real(kind=4), dimension(0:ip+1,0:jp+1,0:kp+1)  :: rhs\n",
    "            real(kind=4), dimension(-1:ip+1,-1:jp+1,0:kp+1)  :: sm\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,0:kp+1)  :: u\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: usum\n",
    "            real(kind=4), dimension(ip,jp,kp)  :: uwfx\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,0:kp+1)  :: v\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: vsum\n",
    "            real(kind=4), dimension(0:ip+1,-1:jp+1,-1:kp+1)  :: w\n",
    "            real(kind=4), dimension(0:ip,0:jp,0:kp)  :: wsum\n",
    "            real(kind=4), dimension(-1:ipmax+1,-1:jpmax+1)  :: zbm\n",
    "\n",
    "* Testing the MPI baseline on the Mac with following settings:\n",
    "\n",
    "        #define NO_GLOBAL_SOR \n",
    "        #define WV_TIMESTEPS 500\n",
    "        #define NO_IO \n",
    "        #define IFBF 1 \n",
    "        #define IADAM 0 \n",
    "        #undef I_IFDATA_OUT\n",
    "        #undef I_AVEFLOW\n",
    "        #undef I_ANIME\n",
    "\n",
    "This gives 375MB per MPI process for 4 processes and a 300x300x80 domain\n",
    "\n",
    "* Testing the improved version on the Mac with following settings:\n",
    "\n",
    "        #define MPI_NEW_WV\n",
    "        #define WV_NEW_FEEDBF\n",
    "        #define WV_NEW_LES\n",
    "        #define WV_NEW_LES2\n",
    "        #define WV_NEW_VEL2\n",
    "        #define WV_NEW_VELFG\n",
    "        #define WV_NEW\n",
    "        #define NO_GLOBAL_SOR \n",
    "        #define WV_TIMESTEPS 500\n",
    "        #define NO_IO \n",
    "        #define IFBF 1 \n",
    "        #define IADAM 0 \n",
    "        #undef I_IFDATA_OUT\n",
    "        #undef I_AVEFLOW\n",
    "        #undef I_ANIME\n",
    "\n",
    "This gives 198MB per MPI process for 4 processes and a 300x300x80 domain, so an improvement of almost a factor of 2. Going purely on the 3-D arrays it should have been closer to 3.3x. Maybe there are some subroutine-local arrays I overlooked? There are 4 arrays in `anime` but that is unused with this setup.\n",
    "\n",
    "\n",
    "\n",
    "I think there are still way too many arrays. I should check if I can get rid of the `ave*` ones. And what about `{u,v,w,}sum`?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Nested LES compiles wiht these new features but is yet to be tested.\n",
    "\n",
    "### WV_NEW Debugging \n",
    "\n",
    "#### (for historical info only)\n",
    "\n",
    "- `WV_NEW` produces quite different results, but this might be a transient, I will have to run for longer.\n",
    "- OK, did that and it NaNs. And I checked without MPI, and it stil NaNs. So there must be an actual mistake. Let's find out. \n",
    "\n",
    "  - The following files have WV_NEW:\n",
    "\n",
    "        adam.f95\n",
    "        anime.f95\n",
    "        aveflow.f95\n",
    "        bondFG.f95\n",
    "        bondv1.f95\n",
    "        boundp.f95\n",
    "        boundsm.f95\n",
    "        feedbf.f95\n",
    "        feedbfm.f95\n",
    "        grid.f95\n",
    "        ifdata.f95\n",
    "        init.f95\n",
    "        les.f95\n",
    "        main.f95\n",
    "        press.f95\n",
    "        set.f95\n",
    "        timseris.f95\n",
    "        vel2.f95\n",
    "        velFG.f95\n",
    "        velnw.f95\n",
    "\n",
    "  - So that is all of them, really. The following are guaranteed OK:\n",
    "\n",
    "        adam.f95\n",
    "        anime.f95\n",
    "        aveflow.f95\n",
    "        bondFG.f95        \n",
    "        bondv1.f95\n",
    "        boundp.f95\n",
    "        boundsm.f95\n",
    "        grid.f95\n",
    "        timseris.f95\n",
    "        velnw.f95\n",
    "        vel2.f95        \n",
    "        \n",
    "  - Most likely OK: (assuming that we did not eliminate too many arrays)\n",
    "\n",
    "        feedbfm.f95\n",
    "        ifdata.f95\n",
    "        init.f95\n",
    "        main.f95        \n",
    "        set.f95 \n",
    "\n",
    "  - Critical subroutines, most likely to contain the bug:\n",
    "\n",
    "        feedbf.f95 => no MPI\n",
    "        les.f95 => MPI only via call to boundsm\n",
    "        press.f95      \n",
    "        velFG.f95 => MPI only via call to bondfg       \n",
    "        \n",
    "    For `feedbf.f95` there is a macro `NOT_INLINED` which I can set to test.       \n",
    "    In `press.f95`, the macro `WV_OPENCL` interacts with `WV_NEW`, so it is possible that is is only wrong without OpenCL.\n",
    "\n",
    "- I also compared 'vanilla' MPI with 'vanilla' non-MPI and the difference in uspd/vspd is very considerable. But it does not NaN.\n",
    "        \n",
    "\n",
    "\n",
    "- See if I can isolate the `WV_NEW` functionality so that testing what is wrong is easier. \n",
    "- Start with `feedbf`, though I doubt that the problem is there. I replaced `WV_NEW` in feedbf with `WV_NEW_FEEDBF`. I will do the same for the changes in velFG, les and press. Unsurprisingly, this still NaNs, so it is not the root cause. \n",
    "- Let's try the same with `les`: `WV_NEW_LES`, again, still NaNs.\n",
    "- Next, `velfg`, so we define `WV_NEW_VELFG`. This does not NaN but the result is completely wrong, so I must have made a mistake in the macros here.\n",
    "- OK, hopefully fixed all that. And now the bug is gone, which means that the cause is in `velfg` I suppose\n",
    "- So now I should be able to switch on `WV_NEW_FEEDBF` and `WV_NEW_LES` and still get the correct results.\n",
    "- For `WV_NEW_FEEDBF` this is OK!\n",
    "- For `WV_NEW_LES` this is not possible because `WV_NEW_LES` eliminates the `diu*` arrays used by `velfg`. So as a first step we need to adapt `velfg` to work without `diu*` only, keeping the other arrays. \n",
    "- I did that, removed arrays in `vel2`, as far as I can tell they are not used outside it, so no point in computing bounds on them. And yes, it works, although the result is a little bit different from the result with the arrays. To be sure, I ran a sim for 500 timesteps, and it's fine.\n",
    "- Meanwhile, lets remove `nou*`, as this is not used outside `velfg` so also not outside `vel2` I guess. I did this using `WV_NEW_LES2`. I ran a sim for 500 timesteps, and it's fine.\n",
    "- Finally, let's look at `cov*`, as the problem must be there (or in the inlined boundary calcs? unlikely!).\n",
    "- I do this in two steps: \n",
    " (1) merge the (2-D) boundary calculations into the 3-D loops, using `WV_NEW_VEL2`. This is fine (run of 100)\n",
    " (2) scalarise the loops to ultimately remove `cov*`, and that should be `WV_NEW_VELFG` I guess.\n",
    " I checked the code and found that I had omitted some boundary conditions (k=1) for cov3 and cov6. But it still NaNs, so back to the drawing board.\n",
    "- OK, now it is fixed. Essentially there were errors in the boundary conditions. I've done a sim for 500 timesteps, and it's fine. Now doing an MPI run for 500 steps, then one for 10000 steps.\n",
    "- However, running 10000 steps with MPI NaNs, after less than 1000 steps.\n",
    "- However however, running 10000 steps without MPI does not NaN. Still, the values are not the same as for the MPI run. This is not so surprising as the point where the values are taken is a bit different.\n",
    "- So now I must find out why the MPI version NaNs. \n",
    "    - I can see no reason in velFG itself.\n",
    "    - feedbf(m) seems OK, all we did is localise the `*mask` arrays\n",
    "    - It is possible that the problem is rather `MPI_NEW_WV` as I only ran this for 50 steps.\n",
    "    - No, I checked running with `MPI_NEW_WV` undefined and it still NaNs\n",
    "    - So I must go back and undef the WV_NEW features. Starting with `WV_NEW_VELFG`\n",
    "    - The outcome is, sadly, that it works. So `WV_NEW_VELFG` must somehow be wrong, but only if we do MPI\n",
    "    \n",
    "        #define MPI_NEW_WV\n",
    "        #undef NESTED_LES\n",
    "        #define WV_NEW_FEEDBF\n",
    "        #define WV_NEW_LES\n",
    "        #define WV_NEW_LES2\n",
    "        #undef WV_NEW_VELFG\n",
    "        #define WV_NEW\n",
    "        #define WV_TIMESTEPS 10000\n",
    "        #define NO_GLOBAL_SOR \n",
    "        #define IFBF 1 \n",
    "        #define IADAM 0 \n",
    "        #undef I_IFDATA_OUT\n",
    "        #undef I_AVEFLOW\n",
    "        #define I_ANIME\n",
    "\n",
    "    - Ah, I think I've got it: the boundary conditions! \n",
    "    \n",
    "###Â Fixing the `WV_NEW_VELFG` case\n",
    "\n",
    "```fortran\n",
    "The original code was:\n",
    "\n",
    "        #if !defined(MPI) || (PROC_PER_ROW==1)\n",
    "              do k = 1,kp\n",
    "              do i = 1,ip\n",
    "                nou2(i,0,k) = nou2(i,jp,k)\n",
    "                nou2(i,jp+1,k) = nou2(i,1,k)\n",
    "              end do\n",
    "              end do\n",
    "        #else\n",
    "            call sideflowRightLeft(nou2, procPerRow, jp+1, 1, 1, 2, 1, 2)\n",
    "            call sideflowLeftRight(nou2, procPerRow, 2, jp+2, 1, 2, 1, 2)\n",
    "        #endif\n",
    "```\n",
    "\n",
    "The current code is:\n",
    " \n",
    "```fortran\n",
    "          do k = 1,kp\n",
    "              do j = 1,jp\n",
    "                  do i = 1,ip\n",
    "                      nou2_ = (dx1(i+1)*v(i,j-1,k)+dx1(i)*v(i+1,j-1,k)) /(dx1(i)+dx1(i+1))\n",
    "                      diu2_ = 2.*(-u(i,j-1,k)+u(i,j,k))/(dy1(j-1)+dy1(j))\n",
    "                      cov2_j = nou2_*diu2_\n",
    "                      if (j<jp) then\n",
    "                          nou2_jp1 = (dx1(i+1)*v(i,j,k)+dx1(i)*v(i+1,j,k)) /(dx1(i)+dx1(i+1))\n",
    "                          diu2_jp1 = 2.*(-u(i,j,k)+u(i,j+1,k))/(dy1(j)+dy1(j+1))\n",
    "                          cov2_jp1 = nou2_jp1*diu2_jp1\n",
    "                      else\n",
    "                          nou2_ = (dx1(i+1)*v(i,0,k)+dx1(i)*v(i+1,0,k)) /(dx1(i)+dx1(i+1))\n",
    "                          diu2_ = 2.*(-u(i,0,k)+u(i,1,k))/(dy1(0)+dy1(1))\n",
    "                          cov2_jp1 = nou2_*diu2_\n",
    "                      end if \n",
    "                  end do          \n",
    "              end do          \n",
    "          end do                    \n",
    "```          \n",
    "\n",
    "This is wrong because if `j==jp` then we can't access the values at j=0 and j=1. So I should copy these values with MPI.\n",
    "\n",
    "So the sender process should compute them: \n",
    "\n",
    "- If we are in the left sideplane, we calculate  \n",
    "\n",
    "          nou2_1 = (dx1(i+1)*v(i,0,k)+dx1(i)*v(i+1,0,k)) /(dx1(i)+dx1(i+1))\n",
    "          diu2_1 = 2.*(-u(i,0,k)+u(i,1,k))/(dy1(0)+dy1(1))\n",
    "          cov2_1 = nou2_1*diu2_1  \n",
    "\n",
    "and we sent this to the right sideplane. We can actually just compute `cov2_` and send it only if `j==1`\n",
    "\n",
    "- If we are in the right sideplane, and `j==jp`, we block to receive `cov2_1`, then we say\n",
    "\n",
    "          cov2_jp1 = cov2_1\n",
    "\n",
    "- This is inefficient, what we should do of course is store this in an  array.\n",
    "\n",
    "- The routine\n",
    "\n",
    "        sideflowLeftRight(array, procPerRow, colToSend, colToRecv, &\n",
    "                topThickness, bottomThickness, ignoreFirstK, ignoreLastK)\n",
    "\n",
    "assumes that array is a 3-D array but we could define \n",
    "\n",
    "        real, dimension(ip,1,kp) :: cov2_left_plane \n",
    "\n",
    "and I think we call it like\n",
    "\n",
    "        sideflowLeftRight(cov2_left_plane, procPerRow,1,1,0,0,0,0)\n",
    "        \n",
    "and then we should use these values to recompute. However, I think a better way is to just update the bounds on f,g and h. The values of u,v,w do not change after this call. So updating f,g,h the way we do should be fine. Fingers crossed.         \n",
    "- Somehow this is wrong, it NaNs now very quickly was before. So just turning off these boundary calculations does not work. I switch the open ones (ip+1 = ip) on in MPI. \n",
    "- This is still not good, which means that my assumptions on the values in u,v,w must be incorrect.\n",
    "- I rewrote some expressions which seem to have uspd/uspd in them, not sure this can make a difference as the compiler should do this, right? And indeed, it does not make a difference.\n",
    "- So it boils down to this then:\n",
    "\n",
    "        nou2_jp1 = (dx1(i+1)*v(i,jp,k)+dx1(i)*v(i+1,jp,k)) /(dx1(i)+dx1(i+1))\n",
    "        diu2_jp1 = 2.*(-u(i,jp,k)+u(i,jp+1,k))/(dy1(jp)+dy1(jp+1))\n",
    "        cov2_jp1 = nou2_jp1*diu2_jp1\n",
    "\n",
    "        ! cov2(i,jp+1,k) = cov2(i,1,k)\n",
    "        #if !defined(MPI) || (PROC_PER_ROW==1)\n",
    "              if (j==jp) then\n",
    "                  nou2_ = (dx1(i+1)*v(i,0,k)+dx1(i)*v(i+1,0,k)) /(dx1(i)+dx1(i+1))\n",
    "                  diu2_ = 2.*(-u(i,0,k)+u(i,1,k))/(dy1(0)+dy1(1))\n",
    "                  cov2_jp1 = nou2_*diu2_\n",
    "              end if\n",
    "        #endif\n",
    "\n",
    "        nou5_jp1 = ( v(i,jp,k)+v(i,jp+1,k))/2.\n",
    "        diu5_jp1 = (-v(i,jp,k)+v(i,jp+1,k))/dy1(jp+1)\n",
    "        cov5_jp1 = nou5_jp1*diu5_jp1\n",
    "\n",
    "        ! cov5(i,jp+1,k) = cov5(i,1,k)\n",
    "        #if !defined(MPI) || (PROC_PER_ROW==1)        \n",
    "              if (j==jp) then\n",
    "                  nou5_ = ( v(i,1,k)+v(i,2,k))/2.\n",
    "                  diu5_ = (-v(i,1,k)+v(i,2,k))/dy1(j)\n",
    "                  cov5_jp1 = nou5_*diu5_\n",
    "              end if  \n",
    "        #endif\n",
    "\n",
    "        nou8_jp1 = (dzn(k+1)*v(i,jp,k)+dzn(k)*v(i,jp,k+1)) /(dzn(k)+dzn(k+1))\n",
    "        diu8_jp1 = 2.*(-w(i,jp,k)+w(i,jp+1,k))/(dy1(jp)+dy1(jp+1))\n",
    "        cov8_jp1 = nou8_jp1*diu8_jp1\n",
    "      \n",
    "        !cov8(i,jp+1,k) = cov8(i,1,k)\n",
    "        #if !defined(MPI) || (PROC_PER_ROW==1)\n",
    "              if (j==jp) then\n",
    "                nou8_ = (dzn(k+1)*v(i,0,k)+dzn(k)*v(i,0,k+1)) /(dzn(k)+dzn(k+1))\n",
    "                diu8_ = 2.*(-w(i,0,k)+w(i,1,k))/(dy1(0)+dy1(1))\n",
    "                cov8_jp1 = nou8_*diu8_\n",
    "              end if\n",
    "        #endif\n",
    "        \n",
    "So my debug strategy is:\n",
    "\n",
    "1. Check if it NaNs without MPI if I turn off all boundary handling => Yes, it does!\n",
    "2. If it does, check if it NaNs without MPI if I turn off only the circular boundary handling => No, it does not, at least not for the first 500 steps. So, what does this mean? That for the case without MPI, the circular boundary conditions do not matter. This is what I thought as bondv1 should actually take care of that. So this seems to indicate that there is a problem with the open condition in MPI!\n",
    "3. So test MPI with proper open conditions, it looks like I mixed up Top and Bottom => Yes, this is fine, up to 4000 steps at least\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Scripts for building and generating input files\n",
    "\n",
    "### SCons build script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (OclBuilder.py, line 28)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-0da7d333f78e>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import OclBuilder\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/wim/SoC_Research/OpenCL/OpenCLIntegration/OclBuilder.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    print \"No such option: \"+optname\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path # for getting environmental variables on the system\n",
    "\n",
    "# Importing OclBuilder, this is not required for ocl=0\n",
    "# If you want to build just the Fortran code without OpenCL support, use SConstruct.F95_only\n",
    "import OclBuilder\n",
    "from OclBuilder import initOcl\n",
    "\n",
    "# Adding path to includes for kernels\n",
    "CWD= os.environ['PWD']\n",
    "OclBuilder.kopts='-cl-mad-enable -cl-fast-relaxed-math -I'+CWD+'/../OpenCL/Kernels/'\n",
    "\n",
    "from OclBuilder import getOpt\n",
    "OclBuilder.opts=Variables()\n",
    "envF=Environment(useF=1)\n",
    "envF=Environment(ENV={'PATH' : os.environ['PATH']})\n",
    "\n",
    "\n",
    "# Basically, it's Linux unless it's OS X\n",
    "if os.uname()[0] == \"Darwin\":\n",
    "        OSX=1\n",
    "        OSFLAG='-DOSX'\n",
    "else:\n",
    "        OSX=0\n",
    "        OSFLAG='-D__LINUX__'\n",
    "\n",
    "\n",
    "# Then build the rest of the code\n",
    "verbose = getOpt('v','Verbose','1')\n",
    "other=''\n",
    "WITH_OCL=''\n",
    "with_ocl= getOpt('ocl','Use OpenCL','0')\n",
    "if with_ocl == '1':\n",
    "    WITH_OCL = '-D_OPENCL_LES_WV'\n",
    "    envF=initOcl(envF)\n",
    "    kernel_opts = envF['KERNEL_OPTS']\n",
    "else:\n",
    "    envF['F95']=os.environ['FC']\n",
    "    envF['LINK']=os.environ['FC']\n",
    "    VERBOSE = '-DVERBOSE'\n",
    "    if verbose == '0':\n",
    "        VERBOSE = ''\n",
    "    other = getOpt('D','Other macro','')\n",
    "    TIMINGS=''\n",
    "    if other !='':\n",
    "        TIMINGS = '-D'+other\n",
    "\n",
    "# GR: MPI specific build config\n",
    "WITH_MPI=''\n",
    "PROC_PER_ROW=''\n",
    "PROC_PER_COL=''\n",
    "USE_NETCDF_OUTPUT=''\n",
    "with_mpi = getOpt('mpi','Use MPI','0')\n",
    "with_nesting = getOpt('nested','Nested Grid','0')\n",
    "if with_mpi == '1':\n",
    "    procPerRow= getOpt('procPerRow', 'Processes Per Row', '2')\n",
    "    procPerCol= getOpt('procPerCol', 'Processes Per Col', '2')\n",
    "    PROC_PER_ROW = '-DPROC_PER_ROW=' + procPerRow\n",
    "    PROC_PER_COL = '-DPROC_PER_COL=' + procPerCol\n",
    "    WITH_MPI = '-DMPI'\n",
    "    USE_NETCDF_OUTPUT='-DUSE_NETCDF_OUTPUT'\n",
    "#    envF['F95']='mpiifort'\n",
    "#    envF['F95']='mpif90'\n",
    "    if OSX==0:    \n",
    "        envF['LINK']=envF['F95']\n",
    "        envF.Append(LIBS=['netcdf']) # for version less than 4.2.0\n",
    "    else:\n",
    "        envF['F95']=os.environ['FC']\n",
    "        envF['LINK']=os.environ['FC']\n",
    "        envF.Append(LIBS=['mpi_mpifh','netcdff']) # for version more than and equal to 4.2.0 \n",
    "else:\n",
    "    if OSX==1:    \n",
    "        envF['F95']=os.environ['FC']\n",
    "        envF['LINK']=os.environ['FC']\n",
    "        envF.Append(LIBS=['netcdff']) # for version more than and equal to 4.2.0 \n",
    "\n",
    "NESTED_LES=''\n",
    "if with_nesting=='1':\n",
    "\tNESTED_LES='-DNESTED_LES'\n",
    "\t\t\n",
    "GR_DEBUG=''\n",
    "gr_debug = getOpt('gr_debug', 'GR Debug', '0')\n",
    "if gr_debug == '1':\n",
    "    GR_DEBUG='-DGR_DEBUG'\n",
    "\n",
    "WV_DEBUG=''\n",
    "wv_debug = getOpt('wv_debug', 'WV Debug', '0')\n",
    "if wv_debug =='1':\n",
    "    WV_DEBUG='-DMPI_NEW_WV'\n",
    "\n",
    "NO_FILE_IO='-DNO_FILE_IO'\n",
    "ICAL = '-DICAL=0'\n",
    "IFBF='-DIFBF=1'\n",
    "IANIME='-DIANIME=1'\n",
    "IADAM='-DIADAM=0'\n",
    "FFLAGS  = [USE_NETCDF_OUTPUT, WITH_MPI, NESTED_LES, GR_DEBUG, WV_DEBUG, PROC_PER_ROW, PROC_PER_COL, WITH_OCL, NO_FILE_IO, ICAL, IFBF,IANIME, IADAM]\n",
    "\n",
    "if with_ocl == '0':\n",
    "#    FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium', VERBOSE,TIMINGS]\n",
    "#     FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium', '-fno-range-check','-fbounds-check','-Wuninitialized','-ffpe-trap=invalid,zero,overflow', VERBOSE,TIMINGS]\n",
    "     FFLAGS += ['-cpp', '-O', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', '-mcmodel=medium','-fbounds-check', VERBOSE,TIMINGS]\n",
    "#    FFLAGS += ['-cpp','-Ofast', '-m64', '-Wall','-ffree-form', '-ffree-line-length-none','-fconvert=big-endian', VERBOSE,TIMINGS]\n",
    "\n",
    "csources=[]\n",
    "CC= os.environ['CC']\n",
    "envC=Environment(CC=CC)\n",
    "if csources:\n",
    "    envC.Library('csubs',csources)\n",
    "\n",
    "fsources = []\n",
    "\n",
    "if with_mpi == '1':\n",
    "    fsources += ['./communication_common.f95', './communication_helper.f95', './communication_helper_integer.f95', './communication_helper_mpi.f95', './communication_helper_real.f95']\n",
    "    \n",
    "if with_nesting == '1':\n",
    "    fsources += ['./nesting_support.f95']\n",
    "\n",
    "if USE_NETCDF_OUTPUT != '':\n",
    "    fsources += ['./module_LES_write_netcdf.f95']\n",
    "\n",
    "fsources+= ['./fortran_helper.f95', './anime.f95','./aveflow.f95','./bondFG.f95','./bondv1.f95','./boundp.f95','./boundsm.f95','./vel2.f95','./velFG.f95','./feedbf.f95','./feedbfm.f95','./les.f95','./grid.f95','./ifdata.f95','./init.f95','./main.f95','./set.f95','./timdata.f95','./common_sn.f95','./params_common_sn.f95']\n",
    "\n",
    "ffsources=[]\n",
    "\n",
    "if with_ocl == '1':\n",
    "    ffsources = fsources + ['./module_LES_conversions.f95','./module_LES_combined_ocl.f95','./oclWrapper.o']\n",
    "else:\n",
    "    ffsources = fsources + ['./adam.f95','./press.f95','./velnw.f95']\n",
    "\n",
    "\n",
    "if with_ocl == '1':\n",
    "    # Linker flags for OclWrapper\n",
    "    OPENCL_DIR=os.environ['OPENCL_DIR']\n",
    "    OCL_LDFLAGS =  ['-L.','-L'+OPENCL_DIR+'/OpenCLIntegration']\n",
    "else:\n",
    "    OCL_LDFLAGS =  []\n",
    "\n",
    "if OSX == 1:\n",
    "# Assuming MacPorts\n",
    "    INCLPATH = ['/opt/local/include','/opt/local/include/openmpi-gcc49/','/opt/local/lib/openmpi-gcc49/']\n",
    "    LIBPATH = ['/opt/local/lib','/opt/local/lib/openmpi-gcc49/']\n",
    "else:\n",
    "# test for devtoolset-2 ... so better use a var $DEVTOOLSETROOT?\n",
    "    if os.path.exists('/opt/rh/devtoolset-2'):\n",
    "        INCLPATH = ['/opt/rh/devtoolset-2/root/usr/include' ]\n",
    "        LIBPATH = '/opt/rh/devtoolset-2/root/usr/lib'\n",
    "    else:\n",
    "# reasonable default ...\n",
    "        NETCDF = os.environ.get('NETCDF_DIR')\n",
    "        INCLPATH = [NETCDF + '/include']\n",
    "        LIBPATH  = [NETCDF + '/lib']\n",
    "#MPICH = os.environ.get('MPICH')\n",
    "#INCLPATH = [NETCDF + '/include', MPICH + '/include']\n",
    "#LIBPATH = [NETCDF + '/lib', MPICH + '/lib']\n",
    "#        INCLPATH = [NETCDF + '/include', '/usr/include']\n",
    "#        LIBPATH  = [NETCDF + '/lib', '/usr/local/lib']\n",
    "#        INCLPATH = ['/usr/local/include', '/usr/include' ]\n",
    "#        LIBPATH = '/usr/local/lib'\n",
    "#INCLPATH += ['../OpenCL','../OpenCL/Wrappers']\n",
    "\n",
    "envF.Append(F95FLAGS=FFLAGS)\n",
    "envF.Append(F95PATH=['.',INCLPATH])\n",
    "envF.Append(LIBPATH=['.',LIBPATH])\n",
    "if OSX != 1:\n",
    "    envF.Append(LIBS=['m'])\n",
    "\n",
    "mpi_ext=''\n",
    "if with_mpi == '1':\n",
    "    mpi_ext='_mpi'\n",
    "\n",
    "ocl_ext=''\n",
    "if with_ocl == '1':\n",
    "    envF.Append(LIBS=['OclWrapperF','stdc++','OclWrapper'])\n",
    "    ocl_ext='_ocl'\n",
    "    if OSX==1:\n",
    "            envF.Append(FRAMEWORKS=['OpenCL'])\n",
    "    else:\n",
    "            envF.Append(LIBS=['OpenCL'])\n",
    "\n",
    "if csources:\n",
    "    envF.Append(LIBS=['csubs'])\n",
    "\n",
    "prog = envF.Program('les_main'+ocl_ext+mpi_ext,ffsources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build and run script\n",
    "\n",
    "`./src/build_and_run_WV_MPI_no_nesting.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 total processes failed to start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: ./les_main_mpi: No such file or directory\n",
      "\n",
      "scons: warning: Ignoring missing SConscript 'SConstruct.mac'\n",
      "File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scons-3.0.1-py2.7.egg/EGG-INFO/scripts/scons\", line 201, in <module>\n",
      "--------------------------------------------------------------------------\n",
      "mpiexec-openmpi-gcc49 was unable to launch the specified application as it could not access\n",
      "or execute an executable:\n",
      "\n",
      "Executable: ./les_main_mpi\n",
      "Node: h134\n",
      "\n",
      "while attempting to start process rank 0.\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#!/bin/sh  \n",
    "\n",
    "rm ./les_main_mpi\n",
    "\n",
    "procPerRow=$1\n",
    "procPerCol=$2\n",
    "\n",
    "scons -s -f SConstruct.mac v=0 gr_debug=0 wv_debug=0 mpi_new_wv=1 nested=0 ocl=0 mpi=1 procPerRow=${procPerRow} procPerCol=${procPerCol} $3\n",
    "\n",
    "mpiexec-openmpi-gcc7 -np $((procPerRow*procPerCol)) ./les_main_mpi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Helper scripts for CPP macro handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In `./aux` there are two scripts to help with CPP macro handling, in particular for the case of running the OpenCL compiler (see [Generating and building the GPU code](#Generating-and-building-the-GPU-code) for full details on this process).\n",
    "The first is `run_cpp.pl` which runs `cpp` with a list of defined macros. The script will look for a file `macros.h` in the directory where it is run (in our case `src`), if this file does not exist it will not run `cpp` and just copy the source files. It will store the output files in `../PostCPP`.\n",
    "\n",
    "Additionally, the script will look for an optional file `macros_to_skip.h` in the directory where it is run (in our case `src`). Blocks of code guarded by these macros will be removed from the code before running `cpp`, and replaced by `continue` lines. This code is store in `../PostCPP/PrePostCPP`.\n",
    "The removed lines are stored in a file `stash.pl` and can be restored using the script `restore_stashed_lines.pl`, which stores the code in `./PostGen`.\n",
    "\n",
    "The typical use is to run `run_cpp.pl` before running the OpenCL compiler `Autoparallel-fortran`, and then run `restore_stashed_lines` on the sources generated by `Autoparallel-fortran`:\n",
    "\n",
    "    [src]$ perl ../aux/run_cpp.pl\n",
    "    [src]$ cd ../PostCPP\n",
    "    [PostCPP]$ AutoParallel-Fortran-exe ./adam.f95 ./bondv1.f95 ./feedbf.f95 ./les.f95 ./press.f95 ./velFG.f95 ./velnw.f95 -out ../GeneratedCode/ -main ./main.f95 -v -plat GPU\n",
    "    [PostCPP]$ cd ../GeneratedCode\n",
    "    [GeneratedCode]$ perl ../aux/restore_stashed_lines.pl ../src/stash.pl \n",
    "    \n",
    "And the final code is placed in `PostGen`.\n",
    "\n",
    "    [GeneratedCode]$ cd PostGen  \n",
    "\n",
    "For convenience there is also a script `pp_stash.pl` which will print out the Fortran code in the file `stash.pl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Overview of CPP macros used in the LES source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### MPI related     \n",
    "\n",
    "To enable MPI in the code:\n",
    "\n",
    "    MPI\n",
    "    \n",
    "Dimensions of the Cartesian process grid\n",
    "\n",
    "    PROC_PER_ROW\n",
    "    PROC_PER_COL    \n",
    "    \n",
    "Changes to original MPI code before 15 July 2017. Mostly use of collective operations instead of individual send/receive pairs.\n",
    "\n",
    "    MPI_NEW_WV\n",
    "    \n",
    "Used in:\n",
    "\n",
    "    anime.f95\n",
    "    aveflow.f95\n",
    "    common_sn.f95\n",
    "    communication_helper_mpi.f95\n",
    "    communication_helper_real.f95\n",
    "    ifdata.f95\n",
    "    main.f95\n",
    "\n",
    "`MPI_NEW_WV2` is only used in `ifdata.f95`, the code guarded by this macro is *not finished*.\n",
    "    \n",
    "### Debugging\n",
    "\n",
    "To switch on debug output:\n",
    "\n",
    "    GR_DEBUG\n",
    "    WV_DEBUG\n",
    "\n",
    "### Testing\n",
    "\n",
    "* `WV_TEST` is used in `set.f95` to limit the simulation time for testing.\n",
    "* `TEST_SMALL_DOMAIN` in `params_common_sn.f95` is *obsolete*, it was used to limit the domain for testing on GPU.\n",
    "    \n",
    "###   Nesting\n",
    "\n",
    "* `NESTED_LES` enables nesting using stretched grids. It is used in:\n",
    "\n",
    "        anime.f95\n",
    "        bondFG.f95\n",
    "        bondv1.f95\n",
    "        boundp.f95\n",
    "        boundsm.f95\n",
    "        communication_helper_real.f95\n",
    "        grid.f95\n",
    "        main.f95\n",
    "        params_common_sn.f95\n",
    "\n",
    "\n",
    "* `NESTED_LES2` in `communication_helper_real.f95` is *obsolete*.\n",
    "* `SAVE_NESTED_GRID_ONLY` in `anime.f95` and `communication_helper_real.f95` is used for code that saves only the inner  nest of the domain. This is *not finished*.\n",
    "    \n",
    "### Performance evaluation    \n",
    "\n",
    "`TIMINGS` prints out timings for each subroutine in the main time loop.\n",
    "    \n",
    "### Feature control\n",
    "\n",
    "The following replace the run-time Fortran conditions\n",
    "\n",
    "    I_ANIME\n",
    "    I_AVEFLOW\n",
    "    I_IFDATA_OUT\n",
    "\n",
    "The following guard code that is no longer used. I think this code is *obsolete* but it is not my code so I keep it.\n",
    "\n",
    "    TIMSERIS_FIXED    \n",
    "    OLD_CODE\n",
    "\n",
    "### I/O control\n",
    "\n",
    "* `USE_NETCDF_OUTPUT` results in generation of NetCDT output files. However, I think they are *broken* as I can't visualise them in either Paraview or Panoply.\n",
    "* `VERBOSE` prints out intermediate results at run time.\n",
    "* `NO_IO` is mainly used to remove I/O commands from the code for GPU code generation.\n",
    "    \n",
    "###  Code variants \n",
    "\n",
    "These are macros that should have no effect on the simulation except for performance.\n",
    "\n",
    "* `NO_GLOBAL_SOR` is used to disable calculation of the SOR error over the full domain. We can do this because the error is not actually used as a criterion for convergence. This is an advantage for both MPI code and GPU code.\n",
    "* `TWINNED_BUFFER` is an essential performance optimization for GPU: it replaces the red-black SOR algorithm with my twinned double buffering algorithm [ieeexplore.ieee.org/abstract/document/7237073/](ieeexplore.ieee.org/abstract/document/7237073/)\n",
    "* `INLINE_BOUND_CALCS` is another GPU variant, the reason is because our OpenCL compiler can't handle calls to subroutines in kernel code.\n",
    "* `WV_NEW`: This macro is used to guard a lot of changes aimed at reducing the memory footprint of the LES, see below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reducing LES memory footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The main reason for reducing the LES memory footprint is to allow larger domains to be simulated on the GPU. However, this should also result in far less communication in the MPI version.\n",
    "\n",
    "The main changes to reduce the footprint were to remove a number of intermediate arrays by replaycing them with inlined computations.\n",
    "\n",
    "The changes to the code are guarded by the macro `WV_NEW`. The main changes are as follows:\n",
    "* I moved the calculation of the mask for the buildings into the SOR algorithm. This eliminates the arrays `mask*` and `cn*`.\n",
    "* I moved the calculations from `vel2` into `velFG` and scalarized them. This eliminates `diu*`, `cov*` and `nou*`.\n",
    "* In `feedbf` I scalarized the calculations involving `fx`, `fy` and `fz`, eliminating these arrays.\n",
    "\n",
    "In total this frees up half of the memory for the OpenCL case and two thirds for the MPI case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generating the GPU version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The process to generate the GPU version consists of several steps and uses an additional compiler.\n",
    "\n",
    "\n",
    "### Installing the `OclWrapper` library\n",
    "\n",
    "This library provides a Fortran wrapper around the OpenCL C API. The library is [on GitHub](https://github.com/wimvanderbauwhede/OpenCLIntegration) and you need the Python build tool [scons](http://scons.org/) to build it. \n",
    "\n",
    "* Clone the repository:\n",
    "\n",
    "        git clone https://github.com/wimvanderbauwhede/OpenCLIntegration\n",
    "\n",
    "* Follow the instructions in the `README.md`, in particular you should put the content of `ocl_env.sh` in your `.bashrc` (Linux) or `.profile` (Mac) and adapt it for your system.\n",
    "\n",
    "* You don't need to build the library, this is done later when the rest of the code is built.\n",
    "\n",
    "\n",
    "### Building and installing the Fortran to GPU OpenCL compiler\n",
    "\n",
    "This compiler generates OpenCL host code using the `OclWrapper` library and produces parallelised kernel code in Fortran.\n",
    "\n",
    "The compiler is [on GitHub](https://github.com/wimvanderbauwhede/AutoParallel-Fortran) and you need the Haskell build tool [stack](https://www.haskellstack.org/) to build it. \n",
    "\n",
    "\n",
    "* Install [stack](https://docs.haskellstack.org/en/stable/README/)\n",
    "* Clone the repository:\n",
    "\n",
    "        git clone https://github.com/wimvanderbauwhede/AutoParallel-Fortran\n",
    "    \n",
    "* In the repository folder, run \n",
    "\n",
    "        stack install\n",
    "        \n",
    "* This will install the compiler executable `AutoParallel-Fortran-exe` in `$HOME/.local/bin/`. I will assume that this path is in your `$PATH`.\n",
    "\n",
    "\n",
    "### Installing the `RefactorF4Acc` compiler\n",
    "\n",
    "The compiler is [on GitHub](https://github.com/wimvanderbauwhede/RefactorF4Acc) and all you need is `Perl`, v5.16 or later. We use it here to translate the Fortran kernel code into OpenCL C code.\n",
    "\n",
    "* Clone the repository:\n",
    "\n",
    "        git clone https://github.com/wimvanderbauwhede/RefactorF4Acc            \n",
    "           \n",
    "* No further installation is necessary. Below, I have assumed that the local repository path is `$HOME/Git/RefactorF4Acc` but you can put it anywhere you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generating and building the GPU code\n",
    "\n",
    "I have created a script that performs all actions described below, so all you need to do is run the following command:\n",
    "\n",
    "    perl ./aux/generate_and_build_GPU_version.pl\n",
    "       \n",
    "in the `MPI-LES` directory. \n",
    "\n",
    "The `macros.h` and `macros_to_skip.h` files in `src` should be up-to-date, so I think there is no need to change anything there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What this script does (for information only)\n",
    "\n",
    "The script takes the following (optional) arguments: \n",
    "\n",
    "        --dev: the target device: CPU or GPU. Default is $plat\n",
    "        --nth: the number of threads per compute unit. Default is $nth  \n",
    "        --nunits: the number of compute units (--nunits). Default is $nunits\n",
    "        --stage: the stage of the conversion: refactor, autopar, convert, build.\n",
    "                 \n",
    "You can provide several stages separated with a comman. \n",
    "If not given, the script will attempt to run all stages in one go. \n",
    "\n",
    "The stages are:\n",
    "\n",
    "    1. refactor: Refactor the F77 code into accelerator-ready F95 code. \n",
    "    The refactoring source-to-source compiler `rf4a` will only run if \n",
    "        - the source files have extension `.f`, `.f77`, `.F` or `.F77`, and \n",
    "        - the configuration file `rf4a.cfg` is present.\n",
    "    2. autopar: Auto-parallelise the host code and generate the kernel in F95 syntax. This step requires the definition of macros used in the code, in two ways:\n",
    "        - Macros to be expanded using the C preprocessor. You must define/undef these in the file `macros.h`. The script will warn if this file is not present.\n",
    "        - Macros enclosing code to be skipped by the compiler. This is used  in particular because the compiler can't handle IO operations.\n",
    "        You must define/undef these in the file `macros_to_skip.h`. The script will warn if this file is not present.\n",
    "    3. convert: Convert the kernel to OpenCL. \n",
    "    The OpenCL kernel code uses two macros, the number of threads per compute unit `NTH` and the number of compute units `NUNITS`. \n",
    "    These can be defined using the --nth and --nunits flags or in the file `macros_kernel.h`.\n",
    "    4. build: Build the OpenCL Fortran code. The code is built using an auto-generated SConstruct file. \n",
    "    You can of course modify this file and build the code manually. The script will not overwrite an existing `Sconstruct.auto` file.\n",
    "\n",
    "\n",
    "#### GPU code generation and build script source \n",
    "\n",
    "The source code of the script is listed below FYI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%perl\n",
    "#!/usr/bin/env perl\n",
    "use v5.10;\n",
    "use strict;\n",
    "use warnings;\n",
    "\n",
    "# add the './aux' path to the list of include paths\n",
    "BEGIN {\n",
    "    push @INC, './aux';\n",
    "};\n",
    "\n",
    "use Cwd;\n",
    "# A module to run the C preprocessor\n",
    "use RunCpp qw( run_cpp );\n",
    "# Mistery module\n",
    "use RestoreStashedLines qw( restore_stashed_lines );\n",
    "# This converts macro definitions from .h file to command line flag macro syntax\n",
    "use MacroFileToCmdLine qw( macro_file_to_cmd_line_str );\n",
    "\n",
    "use Data::Dumper;\n",
    "use Getopt::Long;\n",
    "\n",
    "=pod\n",
    "\n",
    "# Name \n",
    "\n",
    "  generate_and_build_OpenCL_version.pl\n",
    "\n",
    "# Description\n",
    "\n",
    "The purpose of this script is to convert sequential Fortran 77 code to Fortran 95 code with parallelised OpenCL kernels which can be offloaded to a GPU or run multithreaded on the CPU. \n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "# Source code and directory structure requirements\n",
    "\n",
    "The compilers used by the script do not work on all Fortran 77 or Fortran 95 code. However, it is usually possible with little effort to change the code and selected the correct compilation options to make it work.\n",
    "\n",
    "## Source code requirements\n",
    "\n",
    "# Usage\n",
    "=cut\n",
    "\n",
    "my $plat = 'GPU';\n",
    "my $nth = 256;\n",
    "my $nunits = 16;\n",
    "my $vvv=0;\n",
    "\n",
    "my $help_message =<<ENDH;\n",
    "\n",
    "        $0 [-d, --dev CPU|GPU] [-s, --stage stage] [--nth #threads ] [--nunits #units]   \n",
    "\n",
    "    The script takes the following (optional) arguments: \n",
    "        --dev: the target device: CPU or GPU. Default is $plat\n",
    "        --nth: the number of threads per compute unit. Default is $nth  \n",
    "        --nunits: the number of compute units (--nunits). Default is $nunits\n",
    "        --stage: the stage of the conversion: refactor, autopar, convert, build.\n",
    "                 \n",
    "    You can provide several stages separated with a comman. \n",
    "    If not given, the script will attempt to run all stages in one go. \n",
    "\n",
    "    The stages are:\n",
    "\n",
    "    1. refactor: Refactor the F77 code into accelerator-ready F95 code. \n",
    "    The refactoring source-to-source compiler `rf4a` will only run if \n",
    "        - the source files have extension `.f`, `.f77`, `.F` or `.F77`, and \n",
    "        - the configuration file `rf4a.cfg` is present.\n",
    "    2. autopar: Auto-parallelise the host code and generate the kernel in F95 syntax. This step requires the definition of macros used in the code, in two ways:\n",
    "        - Macros to be expanded using the C preprocessor. You must define/undef these in the file `macros.h`. The script will warn if this file is not present.\n",
    "        - Macros enclosing code to be skipped by the compiler. This is used  in particular because the compiler can't handle IO operations.\n",
    "        You must define/undef these in the file `macros_to_skip.h`. The script will warn if this file is not present.\n",
    "    3. convert: Convert the kernel to OpenCL. \n",
    "    The OpenCL kernel code uses two macros, the number of threads per compute unit NTH and the number of compute units NUNITS. \n",
    "    These can be defined using the --nth and --nunits flags or in the file `macros_kernel.h`.\n",
    "    4. build: Build the OpenCL Fortran code. The code is built using an auto-generated SConstruct file. \n",
    "    You can of course modify this file and build the code manually. The script will not overwrite an existing `Sconstruct.auto` file.\n",
    "    \n",
    "ENDH\n",
    "my $help=0;\n",
    "\n",
    "my $stages_str='refactor,autopar,convert,build';\n",
    "my $use_separate_stash_step=0;\n",
    "my $verbose;\n",
    "GetOptions ('nth=i' => \\$nth,                   \n",
    "            'nunits=i' => \\$nunits,\n",
    "            'dev=s'   => \\$plat,     # I know, not consistent.\n",
    "            'stage=s' => \\$stages_str,\n",
    "            'stash' => \\$use_separate_stash_step,\n",
    "            'verbose'  => \\$vvv,\n",
    "            'help' => \\$help\n",
    "        ) or die(\"Error in command line arguments\\n\");\n",
    "\n",
    "if ($help) { die $help_message; }        \n",
    "my $wd = cwd();\n",
    "\n",
    "my $VV=1;\n",
    "my $vflag= $vvv ? '-b' : '';\n",
    "\n",
    "my $main_src = 'main.f95';\n",
    "\n",
    "# TODO: These should be extracted from the source code using rf4a. It would be best to save these to a file when running the refactoring\n",
    "\n",
    "my @kernel_sources=qw(\n",
    "adam.f95\n",
    "bondv1.f95\n",
    "feedbf.f95\n",
    "les.f95\n",
    "press.f95\n",
    "velFG.f95\n",
    "velnw.f95\n",
    ");\n",
    "\n",
    "# TODO: These should be extracted from the source code using rf4a\n",
    "\n",
    "my @iowrite_subs=qw(\n",
    "'anime'\n",
    ");\n",
    "\n",
    "# TODO: These should be extracted from the source code using rf4a, because they are the source used for building the code minus the kernel sources.\n",
    "# However, that would probably result in many unused files being copied. So we could just copy the src folder \n",
    "#bondv1.f95\n",
    "my @orig_sources=qw(\n",
    "anime.f95\n",
    "aveflow.f95\n",
    "bondFG.f95\n",
    "boundp.f95\n",
    "boundsm.f95\n",
    "common_sn.f95\n",
    "feedbfm.f95\n",
    "grid.f95\n",
    "ifdata.f95\n",
    "init.f95\n",
    "params_common_sn.f95\n",
    "set.f95\n",
    "timdata.f95\n",
    "timseris.f95\n",
    "macros.h\n",
    "macros_to_skip.h\n",
    ");\n",
    "\n",
    "#\n",
    "\n",
    "my $iowrite_subs_str = join(' ',@iowrite_subs);\n",
    "my $kernel_sources_str = join(' ',map {\"./$_\" } @kernel_sources);\n",
    "\n",
    "my @sub_names = map {s/\\.f95$//;$_ } @kernel_sources;\n",
    "\n",
    "# This 30 character limit was picked ad-hoc by Gavin\n",
    "my $superkernel_name = substr(join('_',@sub_names),0,30);\n",
    "if (length($superkernel_name)==30) {\n",
    "    $superkernel_name .= \"_etc_superkernel\"\n",
    "} else {\n",
    "    $superkernel_name .= \"_superkernel\"\n",
    "}\n",
    "\n",
    "my $TRUST_THE_COMPILER = 1 - $use_separate_stash_step;\n",
    "\n",
    "my $skip_step_0 = 1;\n",
    "my $skip_step_1 = 1;\n",
    "my $skip_step_2 = 1;\n",
    "my $skip_step_3 = 1;\n",
    "\n",
    "my %stages = map { $_ => $_} split(/\\s*,\\s*/,$stages_str);\n",
    "\n",
    "if (exists $stages{refactor}) {\n",
    "    $skip_step_0 = 0;\n",
    "}\n",
    "if (exists $stages{autopar}) {\n",
    "    $skip_step_1 = 0;\n",
    "}\n",
    "if (exists $stages{convert}) {\n",
    "    $skip_step_2 = 0;\n",
    "}\n",
    "if (exists $stages{build}) {\n",
    "    $skip_step_3 = 0;\n",
    "}\n",
    "\n",
    "my $gen_dir = 'GeneratedCode';\n",
    "if ($TRUST_THE_COMPILER==1) {\n",
    "    $gen_dir = 'GeneratedCodeV2';\n",
    "}\n",
    "\n",
    "# The compiler fails if this directory does not exists\n",
    "if (not -d $gen_dir) {\n",
    "    mkdir $gen_dir;\n",
    "}\n",
    "\n",
    "\n",
    "# this is LES-specific\n",
    "chdir $gen_dir;\n",
    "if (not -d 'data') {\n",
    "    system('cp -r ../data .');\n",
    "}\n",
    "if (not -d 'GIS') {\n",
    "    system('cp -r ../GIS .');\n",
    "}\n",
    "chdir $wd;\n",
    " \n",
    "my $refactored=0;\n",
    "\n",
    "# Stage 1. Check if the code needs to be refactored from F77 to F95\n",
    "# If so, refactor it; if not, say why not.\n",
    "if (not $skip_step_0) {\n",
    "    chdir 'src';\n",
    "    my @f77_sources = glob('*.f77 *.F77 *.f *.F');\n",
    "    my $has_F77_code = scalar @f77_sources > 0;\n",
    "    my $has_rf4a_cfg = -e './rf4a.cfg';\n",
    "    if ( $has_rf4a_cfg and $has_F77_code) {\n",
    "        $refactored=1;\n",
    "\n",
    "        say \"Refactoring F77 code into accelerator-ready F95 code\";\n",
    "        say($ENV{HOME}.'/Git/RefactorF4Acc/bin/'.'refactorF4acc.pl -c ./rf4a.cfg '.$vflag); \n",
    "        system($ENV{HOME}.'/Git/RefactorF4Acc/bin/'.'refactorF4acc.pl -c ./rf4a.cfg '.$vflag); \n",
    " \n",
    "    } else {\n",
    "        say \"Refactoring step stage skipped because already done:\\n\";\n",
    "        if (!$has_rf4a_cfg) {\n",
    "        say \"\\t- No rf4a.cfg file\"; \n",
    "        }\n",
    "        if(!$has_F77_code) {\n",
    "            say \"\\t-No F77 source files\";\n",
    "        }\n",
    "        say '';\n",
    "    }\n",
    "}\n",
    "    my $src_dir = $refactored ? 'RefactoredSources' : 'src';\n",
    "\n",
    "# Stage 2. Run the auto-parallelizing GPU compiler `AutoParallel-Fortran-exe`. The output is stored in `GeneratedCodeV2`\n",
    "if (not $skip_step_1) {\n",
    "    if ($TRUST_THE_COMPILER) {\n",
    "        chdir $src_dir;\n",
    "        ##\n",
    "        say'*NOTE 2018-03-07* \n",
    "        The `AutoParallel-Fortran` compiler has built-in handling of macros via the -D and -X flags. \n",
    "        This generates the same code as when using the `run_cpp.pl` and `restore_stashed_lines.pl` scripts. \n",
    "        ' if 0;\n",
    "        \n",
    "        (my $defined_macros_str, my $undef_macros_str) = macro_file_to_cmd_line_str( './macros.h','-D');\n",
    "        (my $macros_to_skip_str, my $empty_str) = macro_file_to_cmd_line_str('./macros_to_skip.h','-X');\n",
    "        \n",
    "        say(\"AutoParallel-Fortran-exe $kernel_sources_str -out ../$gen_dir/ -iowrite $iowrite_subs_str -main ./$main_src -plat $plat  $defined_macros_str $macros_to_skip_str $vflag\" );\n",
    "        #system('which AutoParallel-Fortran-exe');die;\n",
    "        system(\"AutoParallel-Fortran-exe $kernel_sources_str -out ../$gen_dir/ -iowrite $iowrite_subs_str -main ./$main_src  -plat $plat  $defined_macros_str $macros_to_skip_str $vflag\" );    \n",
    "        \n",
    "    } else {    \n",
    "    \n",
    "        say '* First, in `'.$src_dir.'`, run CPP on the code using the macros in `macros.h` and stash lines guarded with macros from `macros_to_skip.h`. This generates the file `stash.pl`' if $VV;\n",
    "        \n",
    "        chdir $src_dir;\n",
    "        \n",
    "        run_cpp();\n",
    "          \n",
    "        ##\n",
    "        say '* Then, in `PostCPP`, run the OpenCL compiler `AutoParallel-Fortran-exe`. This will take a while and produce a lot of output, which you can ignore.' if $VV;\n",
    "        \n",
    "        chdir $wd;\n",
    "        if (not -d 'PostCPP') {\n",
    "            mkdir 'PostCPP';\n",
    "        }\n",
    "         \n",
    "        chdir 'PostCPP';\n",
    "        say(\"AutoParallel-Fortran-exe $kernel_sources_str -out ../$gen_dir/ -iowrite $iowrite_subs_str -main ./$main_src $vflag -plat $plat\");\n",
    "        system(\"AutoParallel-Fortran-exe $kernel_sources_str -out ../$gen_dir/ -iowrite $iowrite_subs_str -main ./$main_src $vflag -plat $plat\");\n",
    "        \n",
    "        ##\n",
    "        say \"* In '$gen_dir', we restore code segments that were stashed in the previous step\" if $VV;\n",
    "        chdir $wd;\n",
    "        chdir $gen_dir;\n",
    "        \n",
    "        restore_stashed_lines(\"$wd/$src_dir/stash.pl\"); \n",
    "        system('cp ./PostGen/* .');\n",
    "    \n",
    "    }\n",
    "}\n",
    "# Stage 3. Copy non-modified source files and scripts and config files needed to build the OpenCL kernel, and generate the OpenCL kernel\n",
    "if (not $skip_step_2) { \n",
    "    \n",
    "    ##\n",
    "    say \"* In `$gen_dir`, we copy the non-modified source files into the current folder, as well as some scripts and config files needed to build the OpenCL kernel.\" if $VV;\n",
    "    chdir $wd;\n",
    "    chdir $gen_dir;\n",
    "    \n",
    "    \n",
    "    \n",
    "    my $ref_dir = $TRUST_THE_COMPILER ? \"$wd/$src_dir\" : \"$wd/PostCPP\";\n",
    "    for my $src (@orig_sources) {\n",
    "       system(\"cp $ref_dir/$src .\"); \n",
    "    }\n",
    "\n",
    "    ## TODO rf4a_to_C.cfg should be generated\n",
    "    my $rf4a_to_C_cfg = <<ENDCFG;\n",
    "MODULE = module_${superkernel_name}\n",
    "MODULE_SRC = module_${superkernel_name}.f95\n",
    "TOP = ${superkernel_name}\n",
    "KERNEL = ${superkernel_name}\n",
    "PREFIX = .\n",
    "SRCDIRS = .\n",
    "NEWSRCPATH = ./Temp\n",
    "EXCL_SRCS = (module_${superkernel_name}_init|_host|\\\\.[^f])\n",
    "EXCL_DIRS = ./PostCPP,./Temp\n",
    "MACRO_SRC = macros_kernel.h\n",
    "\n",
    "ENDCFG\n",
    "    \n",
    "    open my $CFG, '>', 'rf4a_to_C.cfg';\n",
    "    print $CFG $rf4a_to_C_cfg;\n",
    "    close $CFG;\n",
    "    \n",
    "    my @sources2=qw(\n",
    "    macros_kernel.h\n",
    "    array_index_f2c1d.h\n",
    "    );\n",
    "    \n",
    "    my $ref_dir_2 = \"$wd/aux\";\n",
    "    for my $src (@sources2) {\n",
    "        system(\"cp $ref_dir_2/$src . \");\n",
    "    }\n",
    "\n",
    "    ##\n",
    "    say '* Then we generate the actual OpenCL kernel code using `RefactorF4Acc`' if $VV;\n",
    "    chdir $wd;\n",
    "    chdir $gen_dir;\n",
    "    \n",
    "    my $macros_kernel_src = './macros_kernel.h';\n",
    "    \n",
    "    if (not -e $macros_kernel_src ) {\n",
    "        say \"No `macros_kernel.h` file for the macros NTH and NUNITS\";\n",
    "        say \"Creating one with NTH=$nth and NUNITS=$nunits\";\n",
    "        open my $MKS, '>', 'macros_kernel.h';\n",
    "        say $MKS \"#define NTH $nth\";\n",
    "        say $MKS \"#define NUNITS$nunits\";\n",
    "        close $MKS;\n",
    "\n",
    "    }\n",
    "    say($ENV{HOME}.'/Git/RefactorF4Acc/bin/'.'refactorF4acc.pl '.$vflag.' -P translate_to_OpenCL -c rf4a_to_C.cfg '.$superkernel_name); \n",
    "    system($ENV{HOME}.'/Git/RefactorF4Acc/bin/'.'refactorF4acc.pl '.$vflag.' -P translate_to_OpenCL -c rf4a_to_C.cfg '.$superkernel_name);\n",
    "    system(\"cp  module_$superkernel_name.cl module_${superkernel_name}_ORIG.cl\");\n",
    "\n",
    "    # Unused, for debugging\n",
    "    #open my $MK, '<', $macros_kernel_src or die $!;\n",
    "    #my @ls=<$MK>;\n",
    "    #close $MK;\n",
    "    #my $macros_str=join(\" \",map {\n",
    "    #    $_=~s/\\n//;\n",
    "    #    s/^\\s*//;\n",
    "    #    s/\\s*$//;\n",
    "    #    s/.define\\s*/-D/;\n",
    "    #    s/.undef\\s*/-U/;\n",
    "    #    s/\\s+/=/;\n",
    "    #    $_\n",
    "    #} @ls);\n",
    "    #say(\"cpp $macros_str -I. -P module_$superkernel_name.cl > module_${superkernel_name}_after_CPP_for_debugging.cl\");\n",
    "    #system(\"cpp $macros_str -I. -P module_$superkernel_name.cl > module_${superkernel_name}_after_CPP_for_debugging.cl\");\n",
    "\n",
    "}\n",
    "\n",
    "# Stage 4. Build the host code for the OpenCL kernel\n",
    "if (not $skip_step_3) {\n",
    "##\n",
    "    chdir $wd;\n",
    "    chdir $gen_dir;\n",
    "    \n",
    "    ## SConstruct.auto is generated\n",
    "    create_sconstruct($main_src, \\@kernel_sources, \\@orig_sources, $superkernel_name);\n",
    "    \n",
    "    say  \"Now we can build the OpenCL Fortran host code, setting the number of threads and compute units depending on the GPU\";\n",
    "    say 'Note that the Scons build runs cpp on the kernel for the macros NTH, NUNITS and BARRIER_OK';\n",
    "    say 'Normally these are set via the `nth`, `nunits` and `dev` flags on the scons command line';\n",
    "    say 'But you can also put them in `macros_kernel.h`';\n",
    "    \n",
    "    say(\"scons -f SConstruct.auto -s mcm=m dev=$plat nth=$nth nunits=$nunits\");\n",
    "    system(\"scons -f SConstruct.auto -s mcm=m dev=$plat nth=$nth nunits=$nunits\");\n",
    "}\n",
    "\n",
    "# ----- Helper functions -----\n",
    "# \n",
    "sub create_sconstruct { (my $main_src, my $kernel_sources, my $orig_sources, my $superkernel_name)=@_;\n",
    "\n",
    "    my @host_srcs = map {\n",
    "        my $name = strip_ext($_);\n",
    "        my $host_name = $name.'_host';\n",
    "        my $host_src = \"'$host_name.f95'\";\n",
    "        $host_src\n",
    "    } ($main_src, @{ $kernel_sources } );\n",
    "\n",
    "    my $host_srcs_str = join(',',@host_srcs);\n",
    "    my @q_orig_sources = map { \"'./$_'\" } @{ $orig_sources };\n",
    "    my $orig_srcs_str = join(',',@q_orig_sources);\n",
    "\n",
    "    my $module_init_str = \"'./module_${superkernel_name}_init.f95'\";\n",
    "    my $kernel_src_cl_str = \"'module_${superkernel_name}.cl'\";\n",
    "\n",
    "    if (not -e \"SConstruct.auto\" ) {\n",
    "        open my $SCONS_TEMPL,'<',\"$wd/aux/SConstruct.templ\";    \n",
    "        open my $SCONS,'>', \"SConstruct.auto\";\n",
    "        while (my $line= <$SCONS_TEMPL> ) {\n",
    "            $line=~/__HOST_SRCS__/ && do {\n",
    "                $line=~s/__HOST_SRCS__/$host_srcs_str/;            \n",
    "            }; \n",
    "            $line=~/__MODULE_INIT__/ && do {\n",
    "                $line=~s/__MODULE_INIT__/$module_init_str/;\n",
    "            }; \n",
    "            $line=~/__KERNEL_SRC_CL__/ && do {\n",
    "                $line=~s/__KERNEL_SRC_CL__/$kernel_src_cl_str/;\n",
    "            };\n",
    "            $line=~/__ORIG_SOURCES__/ && do {\n",
    "                $line=~s/__ORIG_SOURCES__/$orig_srcs_str/;\n",
    "            }; \n",
    "            print $SCONS $line;\n",
    "        }\n",
    "        close $SCONS;\n",
    "        close $SCONS_TEMPL;\n",
    "    } else {\n",
    "        say \"SConstruct.auto already exists, not overwriting. Delete or rename the file and run the build stage again.\";\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "sub strip_ext { (my $fn)=@_;\n",
    "    $fn=~s/\\.\\w+$//;\n",
    "    return $fn;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
