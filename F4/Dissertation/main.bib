@article{Davidson2016,
author = {Davidson, Gavin},
file = {:home/james/Documents/Uni/MSci-Project/AutoParallel-Fortran/docs/dissertation{\_}Gavin{\_}Davidson{\_}2016.pdf:pdf},
journal = {University of Glasgow, MSci Dissertation},
mendeley-groups = {Level 5 Proj},
number = {April},
title = {{Domain-Specific Automatic Parallelisation of Scientific Code}},
year = {2016}
}
@article{Watanabe2003,
abstract = {A newly developed, linear baroclinic model (LBM) and its application to the tropical ENSO teleconnection is described. The model, based on primitive equations linearized about the observed, zonally varying basic state in northern winter, involves linear schemes for the cumulus convection, and surface sensible and latent heat fluxes, referred to as the moist LBM. This enables us to solve a steady-state response of the coupled dynamical–convective system to a given SST anomaly, which is fairly different from the conventional dry LBM. Linear representation of the convection is acceptable for a realistic range of SST anomalies, reproducing well the Rossby wave train computed in the conventional LBM forced by a tropical heating. The moist LBM is used to examine the formation mechanisms of an anomalous low-level anticyclone near the Philippines that links El Ni{\~{n}}o with the Asian winter monsoon. Given that the conventional LBM simulates the Philippine Sea anticyclone as a Rossby response to the anomalous diabatic cooling associated with the weakened convection over the Maritime Continent, causes of the convective suppression are examined. Moist LBM experiments forced by observed El Ni{\~{n}}o SST anomalies indicate that a basinwide warming of the Indian Ocean, in addition to SST anomalies in the Pacific, has a considerable impact in weakening the convection over the Maritime Continent through a modulation of the Walker circulation. Observational analysis supports this idea and further suggests that the lagged Indian Ocean response to El Ni{\~{n}}o contributes to determining when the Philippine Sea anticyclone is developed. The moist LBM identified a positive wind–evaporation feedback at work between the Philippine anticyclone and the western Pacific SST anomaly, which might also contribute.},
author = {Watanabe, Masahiro and Jin, Fei Fei},
doi = {10.1175/1520-0442(2003)16<1121:AMLBMC>2.0.CO;2},
file = {::},
isbn = {1520-0442},
issn = {08948755},
journal = {Journal of Climate},
mendeley-groups = {Level 5 Proj},
month = {apr},
number = {8},
pages = {1121--1139},
title = {{A moist linear baroclinic model: Coupled dynamical-convective response to El Ni{\~{n}}o}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0442{\%}282003{\%}2916{\%}3C1121{\%}3AAMLBMC{\%}3E2.0.CO{\%}3B2},
volume = {16},
year = {2003}
}
@article{Burgers2002,
abstract = {The question is addressed whether using unbalanced updates in ocean-data assimilation schemes for seasonal forecasting systems can result in a relatively poor simulation of zonal currents. An assimilation scheme, where temperature observations are used for updating only the density field, is compared to a scheme where updates of density field and zonal velocities are related by geostrophic balance. This is done for an equatorial linear shallow-water model. It is found that equatorial zonal velocities can be detoriated if velocity is not updated in the assimilation procedure. Adding balanced updates to the zonal velocity is shown to be a simple remedy for the shallow-water model. Next, optimal interpolation (OI) schemes with balanced updates of the zonal velocity are implemented in two ocean general circulation models. First tests indicate a beneficial impact on equatorial upper-ocean zonal currents.},
author = {Burgers, G and Malmaseda, M A and Vossepoel, F C and van Oldenborgh, G J and van Leeuwen, P J},
doi = {10.1175/1520-0485-32.9.2509},
file = {::},
isbn = {1520-0485},
issn = {0022-3670},
journal = {J. Phys. Oceanogr.},
mendeley-groups = {Level 5 Proj},
month = {sep},
number = {9},
pages = {2509--2519},
title = {{Balanced Ocean-Data Assimilation near the Equator}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0485(2002)032{\%}3C2509:BODANT{\%}3E2.0.CO;2},
volume = {32},
year = {2002}
}
@article{Brioude2013,
abstract = {The Lagrangian particle dispersion model FLEX-PART was originally (in its first release in 1998) designed for calculating the long-range and mesoscale dispersion of air pollutants from point sources, such as after an accident in a nuclear power plant. In the meantime FLEXPART has evolved into a comprehensive tool for atmospheric transport modeling and analysis. Its application fields were extended from air pollution studies to other topics where atmospheric transport plays a role (e.g., exchange between the strato-sphere and troposphere, or the global water cycle). It has evolved into a true community model that is now being used by at least 35 groups from 14 different countries and is seeing both operational and research applications. The last citable manuscript for FLEXPART is: (Stohl et al., 2005) 1 Updates since FLEXPART version 8.0 In this version 8.2 of FLEXPART the representation of phys-ical processes was improved as well as a number of technical changes and bugfixes implemented. In addition, the program is now released under a GNU GPL license. For the first time, detailed installation instructions are provided to help users getting started with running FLEXPART. A short section on the new Python routines pflexpart for reading FLEXPART output data has been included as well.},
author = {Brioude, J. and Arnold, D. and Stohl, A. and Cassiani, M. and Morton, D. and Seibert, P. and Angevine, W. and Evan, S. and Dingwell, A. and Fast, J. D. and Easter, R. C. and Pisso, I. and Burkhart, J. and Wotawa, G.},
doi = {10.5194/gmd-6-1889-2013},
file = {:home/james/Downloads/Brioude{\_}2013-gmd.pdf:pdf},
isbn = {1991-9603},
issn = {1991959X},
journal = {Geoscientific Model Development},
mendeley-groups = {Level 5 Proj},
month = {nov},
number = {6},
pages = {1889--1904},
title = {{The Lagrangian particle dispersion model FLEXPART-WRF version 3.1}},
url = {https://www.geosci-model-dev.net/6/1889/2013/},
volume = {6},
year = {2013}
}
@misc{Fortran2000,
author = {{Fortran 2000}},
mendeley-groups = {Level 5 Proj},
title = {{NIST Fortran 77 test suite}},
url = {http://www.fortran-2000.com/ArnaudRecipes/fcvs21{\_}f95.html},
urldate = {2018-11-06}
}
@misc{NISTITL,
author = {{NIST ITL}},
mendeley-groups = {Level 5 Proj},
title = {{Fortran Available Test Suites}},
url = {https://www.itl.nist.gov/div897/ctg/fortran{\_}form.htm},
urldate = {2018-11-06}
}
@misc{Xilinx,
author = {Xilinx},
mendeley-groups = {Level 5 Proj},
title = {{Xilinx - Adaptable. Intelligent.}},
url = {https://www.xilinx.com/},
urldate = {2018-11-04}
}
@misc{Library,
author = {Bradley, Thomas},
mendeley-groups = {Level 5 Proj},
title = {{High-Productivity Development: Thrust Parallel Algorithms Library}},
url = {https://thrust.github.io/},
urldate = {2018-11-04}
}
@misc{OpenMP,
author = {OpenMP},
mendeley-groups = {Level 5 Proj},
title = {{Home - OpenMP}},
url = {https://www.openmp.org/},
urldate = {2018-11-04}
}
@inproceedings{Lbhner2001,
abstract = {This paper summarizes the major improvements and developments that have taken place during the last year for FEFLO, a general-purpose CFD code based on adaptive, unstructured grids. All aspects of a comprehensive simulation pipeline: pre-processing, gridding, field solvers and post-processing saw important advances, and are treated. The advent of machines with millions of cores focused a lot of the developments on distributed memory aspects of field solvers and multiphysics modules. Parallel grid generation, particles and flow, parallel interpolation, parallel domain splitting and repartitioning for multiphysics were all considered. Furthermore, timings on Xeon and AMD chips led to the realization that memory transfers play a considerable role as compared to floating point operations, something previous chip generations did not exhibit. This led to a thorough analysis of numerical algorithms with subsequent re-writing. This in-depth analysis determined that at present achievable speeds are limited by the communication between processors (MPI). Even as the number of cores/domains can increase to several hundred thousand, due to communication overheads the wall clock time it takes to update a flowfield is bounded. We expect that as more users get access to hundreds of thousands of cores and the domain size per core starts shrinking, they will also encounter this 'minimum timestep barrier'. {\textcopyright} 2013 by Author.},
address = {Reston, Virigina},
author = {Lbhner, Rainald and Yang, Chi and Cebral, Juan and Soto, Orlando and Camelli, Fernando and Baum, Joseph D and Luo, Hong and Sharov, Dmitri},
booktitle = {39th AIAA Aerospace Sciences Meeting and Exhibit},
doi = {10.2514/6.2001-592},
file = {::},
isbn = {9781624101816},
mendeley-groups = {Level 5 Proj},
month = {jan},
number = {January},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Advances in FEFLO}},
url = {http://arc.aiaa.org/doi/10.2514/6.2001-592},
year = {2001}
}
@misc{NVIDIACorporation,
author = {{NVIDIA Corporation}},
mendeley-groups = {Level 5 Proj},
title = {{CUDA FORTRAN | NVIDIA Developer}},
url = {https://developer.nvidia.com/cuda-fortran},
urldate = {2018-11-04}
}
@article{Norman2015,
abstract = {The porting of a key kernel in the tracer advection routines of the Community Atmosphere Model - Spectral Element (CAM-SE) to use Graphics Processing Units (GPUs) using OpenACC is considered in comparison to an existing CUDA FORTRAN port. The development of the OpenACC kernel for GPUs was substantially simpler than that of the CUDA port. Also, OpenACC performance was about 1.5× slower than the optimized CUDA version. Particular focus is given to compiler maturity regarding OpenACC implementation for modern FORTRAN, and it is found that the Cray implementation is currently more mature than the PGI implementation. Still, for the case that ran successfully on PGI, the PGI OpenACC runtime was slightly faster than Cray. The results show encouraging performance for OpenACC implementation compared to CUDA while also exposing some issues that may be necessary before the implementations are suitable for porting all of CAM-SE. Most notable are that GPU shared memory should be used by future OpenACC implementations and that derived type support should be expanded.},
author = {Norman, Matthew and Larkin, Jeffrey and Vose, Aaron and Evans, Katherine},
doi = {10.1016/j.jocs.2015.04.022},
file = {::},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {CUDA,Climate,GPU,HPC,OpenACC},
mendeley-groups = {Level 5 Proj},
month = {jul},
pages = {1--6},
publisher = {Elsevier},
title = {{A case study of CUDA FORTRAN and OpenACC for an atmospheric climate kernel}},
url = {https://www.sciencedirect.com/science/article/pii/S1877750315000605},
volume = {9},
year = {2015}
}
@inproceedings{Oancea2012,
abstract = {This paper presents a real-world pricing kernel for financial derivatives and evaluates the language and compiler tool chain that would allow expressive, hardware-neutral algorithm implementation and efficient execution on graphics-processing units (GPU). The language issues refer to preserving algorithmic invariants, e.g., inherent parallelism made explicit by map-reduce-scan functional combinators. Efficient execution is achieved by manually; applying a series of generally-applicable compiler transformations that allows the generated-OpenCL code to yield speedups as high as 70x and 540x on a commodity mobile and desktop GPU, respectively. Apart from the concrete speed-ups attained, our contributions are twofold: First, from a language perspective;, we illustrate that even state-of-the-art auto-parallelization techniques are incapable of discovering all the requisite data parallelism when rendering the functional code in Fortran-style imperative array processing form. Second, from a performance perspective;, we study which compiler transformations are necessary to map the high-level functional code to hand-optimized OpenCL code for GPU execution. We discover a rich optimization space with nontrivial trade-offs and cost models. Memory reuse in map-reduce patterns, strength reduction, branch divergence optimization, and memory access coalescing, exhibit significant impact individually. When combined, they enable essentially full utilization of all GPU cores. Functional programming has played a crucial double role in our case study: Capturing the naturally data-parallel structure of the pricing algorithm in a transparent, reusable and entirely hardware-independent fashion; and supporting the correctness of the subsequent compiler transformations to a hardware-oriented target language by a rich class of universally valid equational properties. Given the observed difficulty of automatically parallelizing imperative sequential code and the inherent labor of porting hardware-oriented and -optimized programs, our case study suggests that functional programming technology can facilitate high-level; expression of leading-edge performant portable; high-performance systems for massively parallel hardware architectures. {\textcopyright} 2012 ACM.},
address = {New York, New York, USA},
author = {Oancea, Cosmin E. and Andreetta, Christian and Berthold, Jost and Frisch, Alain and Henglein, Fritz},
booktitle = {Proceedings of the 1st ACM SIGPLAN workshop on Functional high-performance computing - FHPC '12},
doi = {10.1145/2364474.2364484},
file = {::},
isbn = {9781450315777},
keywords = {autoparallelization,functional language,memory coalescing,reduction,strength,tiling},
mendeley-groups = {Level 5 Proj},
number = {Section 2},
pages = {61},
publisher = {ACM Press},
title = {{Financial software on GPUs}},
url = {http://dl.acm.org/citation.cfm?doid=2364474.2364484},
year = {2012}
}
@article{Corrigan2012,
abstract = {A typical large-scale CFD code based on adaptive, edge-based finite-element formulations for the solu- tion of compressible and incompressible flow is taken as a test bed to port such codes to graphics hardware (graphics processing units,GPUs) using semi-automatic techniques. In previouswork, a GPU version of this code was presented, in which, for many run configurations, all mesh-sized loops required throughout time stepping were ported. This approach simultaneously achieves the fine-grained parallelism required to fully exploit the capabilities of many-core GPUs, completely avoids the crippling bottleneck of GPU–CPU data transfer, and uses a transposed memory layout to meet the distinct memory access requirements posed by GPUs. The present work describes the next step of this porting effort, namely to integrate GPU-based, fine- grained parallelism with Message-Passing-Interface-based, coarse-grained parallelism, in order to achieve a code capable of running on multi-GPU clusters. This is carried out in a semi-automated fashion: the exist- ing Fortran–Message Passing Interface code is preserved, with the translator inserting data transfer calls as required. Performance benchmarks indicate up to a factor of 2 performance advantage of the NVIDIA Tesla M2050 GPU (Santa Clara, CA, USA) over the six-core Intel Xeon X5670 CPU (Santa Clara, CA, USA), for certain run configurations. In addition, good scalability is observed when running across multiple GPUs. The approach should be of general interest, as how best to run on GPUs is being presently considered for many so-called legacy codes.},
archivePrefix = {arXiv},
arxivId = {DOI: 10.1002/fld.1},
author = {Corrigan, Andrew and L{\"{o}}hner, Rainald},
doi = {10.1002/fld.2664},
eprint = {fld.1},
file = {::},
isbn = {02712091 10970363},
issn = {02712091},
journal = {International Journal for Numerical Methods in Fluids},
keywords = {CFD,GPUs,Supercomputing hardware},
mendeley-groups = {Level 5 Proj},
month = {may},
number = {11},
pages = {1786--1796},
pmid = {18628456},
primaryClass = {DOI: 10.1002},
publisher = {Wiley-Blackwell},
title = {{Semi-automatic porting of a large-scale CFD code to multi-graphics processing unit clusters}},
url = {http://doi.wiley.com/10.1002/fld.2560},
volume = {69},
year = {2012}
}
@article{VanderbauwhedeNabi2018,
abstract = {There is a large body of legacy scientific code written in languages like Fortran that is not optimised to get the best performance out of heterogeneous acceleration devices like GPUs and FPGAs, and manually porting such code into parallel languages frameworks like OpenCL requires considerable effort. We are working towards developing a turn-key, self-optimising compiler for accelerating scientific applications, that can automatically transform legacy code into a solution for heterogeneous targets. In this paper we focus on FPGAs as the acceleration devices, and carry out our discussion in the context of the OpenCL programming framework. We show a route to automatic creation of kernels which are optimised for execution in a "streaming" fashion, which gives optimal performance on FPGAs. We use a 2D shallow-water model as an illustration; specifically we show how the use of $\backslash$emph{\{}channels{\}} to communicate directly between peer kernels and the use of on-chip memory to create stencil buffers can lead to significant performance improvements. Our results show better FPGA performance against a baseline CPU implementation, and better energy-efficiency against both CPU and GPU implementations.},
archivePrefix = {arXiv},
arxivId = {1901.00416},
author = {Vanderbauwhede, Wim and Nabi, Syed Waqar},
eprint = {1901.00416},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanderbauwhede, Nabi - 2018 - Towards Automatic Transformation of Legacy Scientific Code into OpenCL for Optimal Performance on FPGAs.pdf:pdf},
keywords = {accelerators,compiler,figure 1,fpga,gpu,high-performance computing,literature mentions of different,opencl,puting,revisions of fortran,sciencedirect,scientific com-,using google scholar and},
month = {dec},
pages = {1--9},
title = {{Towards Automatic Transformation of Legacy Scientific Code into OpenCL for Optimal Performance on FPGAs}},
url = {http://arxiv.org/abs/1901.00416},
year = {2018}
}

@misc{IntelCorporation,
author = {{Intel Corporation}},
mendeley-groups = {Level 5 Proj},
title = {{FPGA Design Software - Intel{\textregistered} Quartus{\textregistered} Prime}},
url = {https://www.intel.com/content/www/us/en/software/programmable/quartus-prime/overview.html},
urldate = {2018-10-31}
}
@article{Nakayama2011,
abstract = {This paper describes aerodynamic roughness properties for turbulent flows over various building arrays that represent realistic urban surface geometries. First, building morphological characteristics such as roughness density $\gamma$f and building height variability Vh, defined respectively as the ratio of total frontal area of roughness elements to the total surface area and the ratio of standard deviation in building height to the average building height of the study site, were investigated. Next, large-eddy simulations (LESs) of turbulent flows over building arrays were performed with various surface geometries characterized by a wide range of values for both $\gamma$f and Vh, based on this building morphological analysis. Third, aerodynamic roughness parameters such as roughness length z0 and drag coefficient Cd were evaluated for the central Tokyo area from the values of lf and Vh using the LES results. The values of z0 and Cd as a function of both lf and Vh were comparable to those found in earlier studies. The values of z0 and Cd evaluated by a conventional method using only $\gamma$f were underestimated, particularly for densely built-up areas. This indicates that the present approach to estimating aerodynamic roughness parameters, taking account of both roughness density and building height variability, is more appropriate than conventional approaches when applied to actual urban areas. The roughness aerodynamic parameters as a function of lf and Vh obtained from the LES results will be useful in incorporating urban effects into weather forecasting models. {\textcopyright} 2011 American Meteorological Society.},
author = {Nakayama, Hiromasa and Takemi, Tetsuya and Nagai, Haruyasu},
doi = {10.1175/2011JAMC2567.1},
file = {::},
issn = {15588424},
journal = {Journal of Applied Meteorology and Climatology},
keywords = {Large eddy simulations,Turbulence},
mendeley-groups = {Level 5 Proj},
month = {aug},
number = {8},
pages = {1692--1712},
title = {{LES analysis of the aerodynamic surface properties for turbulent flows over building arrays with various geometries}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/2011JAMC2567.1},
volume = {50},
year = {2011}
}
@incollection{Hall2009,
abstract = {Offers an introduction to computer-based modelling of oceanic processes. This title contains over twenty practical exercises, using freely available Open-Source software, and covers a range of topics from long surface waves, geostrophic flows, through to the general wind-driven circulation including western boundary currents and mesoscale eddies. Requirements -- Motivation -- Basics of Geophysical Fluid Dynamics -- Long Waves in a Channel -- 2D Shallow-Water Modelling -- Rotational Effects.},
author = {K{\"{a}}mpf, Jochen},
booktitle = {Ocean Modelling for Beginners},
doi = {10.1002/fld.992.Froude},
isbn = {9783642008207},
mendeley-groups = {Level 5 Proj},
pages = {136--139},
publisher = {Springer-Verlag},
title = {{07{\_}Bibliography}},
url = {https://www.researchgate.net/publication/261174088{\_}Ocean{\_}Modelling{\_}for{\_}Beginners{\_}-{\_}Using{\_}Open-Source{\_}Software},
volume = {17},
year = {2009}
}
@inproceedings{Overbey2005,
abstract = {Not since the advent of the integrated development environment has a development tool had the impact on programmer productivity that refactoring tools have had for object-oriented developers. However, at the present time, such tools do not exist for high-performance languages such as C and Fortran; moreover, refactorings specific to high-performance and parallel computing have not yet been adequately examined. We observe that many common refactorings for object-oriented systems have clear analogs in procedural Fortran. The Fortran language itself and the introduction of object orientation in Fortran 2003 give rise to several additional refactorings. Moreover, we conjecture that many hand optimizations common in supercomputer programming can be automated by a refactoring engine but deferred until build time in order to preserve the maintainability of the original code base. Finally, we introduce Photran, an integrated development environment that will be used to implement these transformations, and discuss the impact of such a tool on legacy code reengineering.This work is being funded by IBM under the PERCS project.},
address = {New York, New York, USA},
author = {Overbey, Jeffrey and Xanthos, Spiros and Johnson, Ralph and Foote, Brian},
booktitle = {Proceedings of the second international workshop on Software engineering for high performance computing system applications  - SE-HPCS '05},
doi = {10.1145/1145319.1145331},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Overbey et al. - 2005 - Refactorings for Fortran and high-performance computing.pdf:pdf},
isbn = {1595931171},
issn = {02705257},
keywords = {Fortran programming,refactoring},
mendeley-groups = {Level 5 Proj},
pages = {37},
publisher = {ACM Press},
title = {{Refactorings for Fortran and high-performance computing}},
url = {http://portal.acm.org/citation.cfm?doid=1145319.1145331},
year = {2005}
}
@inproceedings{Orchard2013,
abstract = {Many of the computer models used in scientific research have been developed in Fortran over many years. This evo- lutionary process means these models often use deprecated language features and idioms that impede software main- tenance, understandability, extension, and verification. To mitigate this, we built CamFort, an open-source automatic refactoring tool for upgrading Fortran source code. We de- scribe functionality in CamFort for removing equivalence statements and common blocks, and for introducing struc- tured data types, and give examples of how these transfor- mations can benefit codebase robustness.},
address = {New York, New York, USA},
author = {Orchard, Dominic and Rice, Andrew},
booktitle = {Proceedings of the 2013 ACM workshop on Workshop on refactoring tools - WRT '13},
doi = {10.1145/2541348.2541356},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Orchard, Rice - 2013 - Upgrading fortran source code using automatic refactoring.pdf:pdf},
isbn = {9781450326049},
keywords = {computational science,fortran,haskell,language evolution,refactoring},
mendeley-groups = {Level 5 Proj},
pages = {29--32},
publisher = {ACM Press},
title = {{Upgrading fortran source code using automatic refactoring}},
url = {http://dl.acm.org/citation.cfm?doid=2541348.2541356},
year = {2013}
}
@inproceedings{Wang2017,
abstract = {Iterative stencil algorithms find applications in a wide range of domains. FPGAs have long been adopted for computa-tion acceleration due to its advantages of dedicated hard-ware design. Hence, FPGAs are a compelling alternative for executing iterative stencil algorithms. However, efficient implementation of iterative stencil algorithms on FPGAs is very challenging due to the data dependencies between iter-ations and elements in the stencil algorithms, programming hurdle of FPGAs, and large design space. In this paper, we present a comprehensive framework that synthesizes iterative stencil algorithms on FPGAs efficiently. We leverage the OpenCL-to-FPGA toolchain to generate accelerator automatically and perform design space explo-ration at the high level. We propose to bridge the neigh-boring tiles through pipe and enable data sharing among them to improve computation efficiency. Then, we extend the equal tile size design to a heterogeneous design with dif-ferent tile size to balance the computation among different tiles. We also develop analytical performance models to ex-plore the complex design space. Experiments using a wide range of stencil applications demonstrate that on average our heterogeneous implementations achieve 1.65X performance speedup but with less hardware resource compared to the state-of-the-art.},
address = {New York, New York, USA},
author = {Wang, Shuo and Liang, Yun},
booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
doi = {10.1145/3061639.3062185},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Liang - 2017 - A Comprehensive Framework for Synthesizing Stencil Algorithms on FPGAs using OpenCL Model.pdf:pdf},
isbn = {9781450349277},
issn = {0738100X},
mendeley-groups = {Level 5 Proj},
pages = {28:1----28:6},
publisher = {ACM Press},
title = {{A Comprehensive Framework for Synthesizing Stencil Algorithms on FPGAs Using OpenCL Model}},
url = {http://doi.acm.org/10.1145/3061639.3062185},
year = {2017}
}
@article{Momeni2016,
author = {Momeni, Amir and Tabkhi, Hamed and Ukidave, Yash and Schirner, Gunar and Kaeli, David},
doi = {10.1145/2927964.2927974},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Momeni et al. - 2016 - Exploring the Efficiency of the OpenCL Pipe Semantic on an FPGA.pdf:pdf},
issn = {0163-5964},
journal = {SIGARCH Comput. Archit. News},
mendeley-groups = {Level 5 Proj},
month = {apr},
number = {4},
pages = {52--57},
publisher = {ACM},
title = {{Exploring the Efficiency of the OpenCL Pipe Semantic on an FPGA}},
url = {http://doi.acm.org/10.1145/2927964.2927974},
volume = {43},
year = {2016}
}
@article{VanderbauwhedeDavidson2018,
abstract = {Massively parallel accelerators such as GPGPUs, manycores and FPGAs represent a powerful and affordable tool for scientists who look to speed up simulations of complex systems. However, porting code to such devices requires a detailed understanding of heterogeneous programming tools and effective strategies for parallelization. In this paper we present a source to source compilation approach with whole-program analysis to automatically transform single-threaded FORTRAN 77 legacy code into OpenCL-accelerated programs with parallelized kernels. The main contributions of our work are: (1) whole-source refactoring to allow any subroutine in the code to be offloaded to an accelerator. (2) Minimization of the data transfer between the host and the accelerator by eliminating redundant transfers. (3) Pragmatic auto-parallelization of the code to be offloaded to the accelerator by identification of parallelizable maps and reductions. We have validated the code transformation performance of the compiler on the NIST FORTRAN 78 test suite and several real-world codes: the Large Eddy Simulator for Urban Flows, a high-resolution turbulent flow model; the shallow water component of the ocean model Gmodel; the Linear Baroclinic Model, an atmospheric climate model and Flexpart-WRF, a particle dispersion simulator. The automatic parallelization component has been tested on as 2-D Shallow Water model (2DSW) and on the Large Eddy Simulator for Urban Flows (UFLES) and produces a complete OpenCL-enabled code base. The fully OpenCL-accelerated versions of the 2DSW and the UFLES are resp. 9x and 20x faster on GPU than the original code on CPU, in both cases this is the same performance as manually ported code.},
archivePrefix = {arXiv},
arxivId = {1711.04471},
author = {Vanderbauwhede, Wim and Davidson, Gavin},
doi = {10.1016/j.compfluid.2018.06.005},
eprint = {1711.04471},
file = {:home/james/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanderbauwhede, Davidson - 2018 - Domain-specific acceleration and auto-parallelization of legacy scientific code in FORTRAN 77 using so.pdf:pdf},
issn = {00457930},
journal = {Computers and Fluids},
keywords = {Acceleration,Auto-parallelization,Fortran,GPGPU,OpenCL,Source-to-source compilation},
mendeley-groups = {Level 5 Proj},
month = {sep},
pages = {1--5},
publisher = {Pergamon},
title = {{Domain-specific acceleration and auto-parallelization of legacy scientific code in FORTRAN 77 using source-to-source compilation}},
url = {https://www.sciencedirect.com/science/article/pii/S0045793018302950},
volume = {173},
year = {2018}
}
